//! Result and scoring types for forgetest.
//!
//! These types capture the outcome of running an eval case: compilation results,
//! test execution, clippy analysis, timing, token usage, and computed scores.

use serde::{Deserialize, Serialize};
use uuid::Uuid;

use crate::model::Expectations;

/// The result of running one eval case against one model.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvalResult {
    /// The eval case ID this result is for.
    pub case_id: String,
    /// The model used (e.g. "claude-sonnet-4-20250514").
    pub model: String,
    /// The provider used (e.g. "anthropic").
    pub provider: String,
    /// The code generated by the LLM.
    pub generated_code: String,
    /// Compilation result.
    pub compilation: CompilationResult,
    /// Test execution result (if tests were run).
    #[serde(default)]
    pub test_execution: Option<TestResult>,
    /// Clippy analysis result (if clippy was run).
    #[serde(default)]
    pub clippy: Option<ClippyResult>,
    /// Timing information.
    pub timing: TimingInfo,
    /// Token usage for this generation.
    pub token_usage: TokenUsage,
    /// Which attempt this is (for Pass@k sampling).
    pub attempt: u32,
    /// Unique run identifier.
    pub run_id: Uuid,
}

/// Result of compiling generated code.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CompilationResult {
    /// Whether compilation succeeded.
    pub success: bool,
    /// Compiler errors.
    #[serde(default)]
    pub errors: Vec<CompilerDiagnostic>,
    /// Compiler warnings.
    #[serde(default)]
    pub warnings: Vec<CompilerDiagnostic>,
    /// Time spent compiling in milliseconds.
    pub duration_ms: u64,
}

/// A single compiler diagnostic (error or warning).
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CompilerDiagnostic {
    /// Severity level.
    pub level: DiagnosticLevel,
    /// The diagnostic message.
    pub message: String,
    /// Rust error code (e.g. "E0308").
    #[serde(default)]
    pub code: Option<String>,
    /// Source locations related to this diagnostic.
    #[serde(default)]
    pub spans: Vec<DiagnosticSpan>,
}

/// Severity level of a compiler diagnostic.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum DiagnosticLevel {
    Error,
    Warning,
    Note,
    Help,
}

/// A source location in a diagnostic.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DiagnosticSpan {
    /// File path.
    pub file: String,
    /// Starting line number.
    pub line_start: u32,
    /// Ending line number.
    pub line_end: u32,
    /// Starting column.
    pub column_start: u32,
    /// Ending column.
    pub column_end: u32,
    /// The source text at this span.
    #[serde(default)]
    pub text: Option<String>,
}

/// Result of running tests.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TestResult {
    /// Number of tests that passed.
    pub passed: u32,
    /// Number of tests that failed.
    pub failed: u32,
    /// Number of tests that were ignored.
    pub ignored: u32,
    /// Time spent running tests in milliseconds.
    pub duration_ms: u64,
    /// Details of each test failure.
    #[serde(default)]
    pub failures: Vec<TestFailure>,
}

/// Details of a single test failure.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TestFailure {
    /// Test function name.
    pub name: String,
    /// Failure message (assertion message, panic message, etc.).
    pub message: String,
    /// Captured stdout from the test.
    #[serde(default)]
    pub stdout: String,
}

/// Result of running clippy.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ClippyResult {
    /// Clippy warnings.
    pub warnings: Vec<CompilerDiagnostic>,
    /// Total warning count.
    pub warning_count: u32,
}

/// Timing breakdown for an eval run.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TimingInfo {
    /// Time spent waiting for the LLM response in milliseconds.
    pub llm_request_ms: u64,
    /// Time spent compiling in milliseconds.
    pub compilation_ms: u64,
    /// Time spent running tests in milliseconds.
    pub test_execution_ms: u64,
    /// Total wall-clock time in milliseconds.
    pub total_ms: u64,
}

/// Token usage for an LLM request.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TokenUsage {
    /// Number of tokens in the prompt.
    pub prompt_tokens: u32,
    /// Number of tokens in the completion.
    pub completion_tokens: u32,
    /// Total tokens (prompt + completion).
    pub total_tokens: u32,
    /// Estimated cost in USD.
    pub estimated_cost_usd: f64,
}

/// Computed score for an eval result.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Score {
    /// Compilation score: 1.0 if compiled, 0.0 otherwise.
    pub compilation: f64,
    /// Test score: ratio of passed tests (0.0-1.0).
    pub tests: f64,
    /// Clippy score: 1.0 minus penalty per warning (0.0-1.0).
    pub clippy: f64,
    /// Weighted overall score.
    pub overall: f64,
}

impl Score {
    /// Compute the score for an eval result given the expectations.
    ///
    /// Scoring:
    /// - Compilation: 1.0 if succeeded, 0.0 if failed
    /// - Tests: ratio of passed / total (0.0 if no tests run)
    /// - Clippy: 1.0 - 0.1 per warning, capped at 0.0
    /// - Overall: weighted average (compilation 40%, tests 50%, clippy 10%)
    pub fn compute(result: &EvalResult, expectations: &Expectations) -> Score {
        let compilation = if result.compilation.success { 1.0 } else { 0.0 };

        let tests = if expectations.should_pass_tests {
            match &result.test_execution {
                Some(test_result) => {
                    let total = test_result.passed + test_result.failed;
                    if total == 0 {
                        0.0
                    } else {
                        test_result.passed as f64 / total as f64
                    }
                }
                None => 0.0,
            }
        } else {
            1.0
        };

        let clippy = match &result.clippy {
            Some(clippy_result) => (1.0 - clippy_result.warning_count as f64 * 0.1).max(0.0),
            None => 1.0,
        };

        // If compilation failed, everything is 0
        let overall = if compilation == 0.0 {
            0.0
        } else {
            compilation * 0.4 + tests * 0.5 + clippy * 0.1
        };

        Score {
            compilation,
            tests,
            clippy,
            overall,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    fn make_result(
        compile_success: bool,
        test_passed: u32,
        test_failed: u32,
        clippy_warnings: u32,
    ) -> EvalResult {
        EvalResult {
            case_id: "test".into(),
            model: "test-model".into(),
            provider: "test".into(),
            generated_code: String::new(),
            compilation: CompilationResult {
                success: compile_success,
                errors: vec![],
                warnings: vec![],
                duration_ms: 100,
            },
            test_execution: if test_passed + test_failed > 0 {
                Some(TestResult {
                    passed: test_passed,
                    failed: test_failed,
                    ignored: 0,
                    duration_ms: 50,
                    failures: vec![],
                })
            } else {
                None
            },
            clippy: Some(ClippyResult {
                warnings: vec![],
                warning_count: clippy_warnings,
            }),
            timing: TimingInfo {
                llm_request_ms: 1000,
                compilation_ms: 100,
                test_execution_ms: 50,
                total_ms: 1150,
            },
            token_usage: TokenUsage {
                prompt_tokens: 100,
                completion_tokens: 200,
                total_tokens: 300,
                estimated_cost_usd: 0.01,
            },
            attempt: 1,
            run_id: Uuid::nil(),
        }
    }

    #[test]
    fn score_perfect() {
        let result = make_result(true, 5, 0, 0);
        let score = Score::compute(&result, &Expectations::default());
        assert_eq!(score.compilation, 1.0);
        assert_eq!(score.tests, 1.0);
        assert_eq!(score.clippy, 1.0);
        assert!((score.overall - 1.0).abs() < f64::EPSILON);
    }

    #[test]
    fn score_compilation_failure() {
        let result = make_result(false, 0, 0, 0);
        let score = Score::compute(&result, &Expectations::default());
        assert_eq!(score.compilation, 0.0);
        assert_eq!(score.overall, 0.0);
    }

    #[test]
    fn score_partial_tests() {
        let result = make_result(true, 3, 2, 0);
        let score = Score::compute(&result, &Expectations::default());
        assert_eq!(score.compilation, 1.0);
        assert!((score.tests - 0.6).abs() < f64::EPSILON);
    }

    #[test]
    fn score_clippy_warnings() {
        let result = make_result(true, 5, 0, 3);
        let score = Score::compute(&result, &Expectations::default());
        assert!((score.clippy - 0.7).abs() < f64::EPSILON);
    }

    #[test]
    fn score_clippy_many_warnings_floors_at_zero() {
        let result = make_result(true, 5, 0, 15);
        let score = Score::compute(&result, &Expectations::default());
        assert_eq!(score.clippy, 0.0);
    }

    #[test]
    fn score_serde_roundtrip() {
        let result = make_result(true, 5, 0, 0);
        let score = Score::compute(&result, &Expectations::default());
        let json = serde_json::to_string(&score).unwrap();
        let deserialized: Score = serde_json::from_str(&json).unwrap();
        assert!((deserialized.overall - score.overall).abs() < f64::EPSILON);
    }
}
